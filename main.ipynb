{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60bc994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in ./.venv/lib/python3.14/site-packages (6.4.0)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.14/site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.14/site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.14/site-packages (1.12.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.14/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.14/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.14/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.14/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.14/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.14/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.14/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.14/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.14/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.14/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.14/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.14/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.14/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.14/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.14/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.14/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.14/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.14/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.14/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.14/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.14/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.14/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.14/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.14/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.14/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.14/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.14/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.14/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.14/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.14/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.14/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Install these and other requirements python may ask\n",
    "!pip install pypdf transformers datasets accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2830bc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted and saved to: combined_documents.txt\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - Extract text from pdf\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_text_from_pdfs(pdf_dir=\"data/pdfs\"):\n",
    "    \"\"\"Extracts text from all PDFs in a directory and returns a single large string.\"\"\"\n",
    "    all_text = \"\"\n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(pdf_dir, filename)\n",
    "            try:\n",
    "                reader = PdfReader(filepath)\n",
    "                for page in reader.pages:\n",
    "                    all_text += page.extract_text() + \"\\\\n\"\n",
    "            except Exception as e:\n",
    "                print(f\"Could not read {filename}: {e}\")\n",
    "\n",
    "    # Save the combined text to a file for easy loading later\n",
    "    with open(\"combined_documents.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(all_text)\n",
    "\n",
    "    return \"combined_documents.txt\"\n",
    "\n",
    "# Assume your PDFs are in a folder named 'data/pdfs'\n",
    "text_file_path = extract_text_from_pdfs()\n",
    "print(f\"Text extracted and saved to: {text_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21159121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 1 examples [00:00, 161.31 examples/s]\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Map (num_proc=1): 100%|██████████| 1/1 [00:00<00:00,  9.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - Load text into Dataset, load GPT-2, Define Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load the text file into a Hugging Face Dataset\n",
    "# We use 'text' dataset builder which loads a file line-by-line\n",
    "raw_datasets = load_dataset('text', data_files={'train': 'combined_documents.txt'})\n",
    "\n",
    "# 2. Load the GPT-2 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add a padding token to GPT-2 (necessary for batch processing, though not strictly required\n",
    "# for Causal Language Modeling if using DataCollatorForLanguageModeling)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. Tokenization Function\n",
    "def tokenize_function(examples):\n",
    "    # The key in the dataset is 'text' since we used the 'text' dataset loader\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4, # Use multiple processes for faster tokenization\n",
    "    remove_columns=raw_datasets[\"train\"].column_names # Remove original text column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595874ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Map (num_proc=1): 100%|██████████| 1/1 [00:00<00:00, 10.67 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset too small (1 blocks) for 95/5 split. Using all blocks for both train and eval.\n",
      "Total training blocks: 1\n",
      "Total evaluation blocks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare\n",
    "block_size = 1024 # Standard block size for GPT-2\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # The line that drops the last small block is correctly commented out:\n",
    "    # total_length = (total_length // block_size) * block_size \n",
    "\n",
    "    # Split by block_size, allowing the last one to be < block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Add 'labels' for CLM, where labels are the input_ids shifted\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "# --- CORRECTED SPLIT LOGIC ---\n",
    "# Get the grouped dataset (which currently only has a 'train' split)\n",
    "main_dataset = lm_datasets['train']\n",
    "total_blocks = len(main_dataset)\n",
    "\n",
    "# If we have very few blocks (less than 2), train_test_split will fail.\n",
    "# Use the whole set for both train and validation in this case.\n",
    "if total_blocks < 2:\n",
    "    print(f\"Dataset too small ({total_blocks} blocks) for 95/5 split. Using all blocks for both train and eval.\")\n",
    "    train_dataset = main_dataset\n",
    "    eval_dataset = main_dataset\n",
    "else:\n",
    "    # Perform the split for larger datasets\n",
    "    split_datasets = main_dataset.train_test_split(test_size=0.05)\n",
    "    train_dataset = split_datasets[\"train\"]\n",
    "    eval_dataset = split_datasets[\"test\"]\n",
    "    \n",
    "print(f\"Total training blocks: {len(train_dataset)}\")\n",
    "print(f\"Total evaluation blocks: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "026ef5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "from transformers import GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "\n",
    "# Load the PRE-TRAINED GPT-2 model\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Data Collator: Prepares batches of data for the model\n",
    "# mlm=False is crucial for Causal Language Modeling (CLM)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca439d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"gpt2-finetuned-custom-docs\"\n",
    "logging_steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    # Core Training Parameters\n",
    "    num_train_epochs=20,                     # Number of epochs to run\n",
    "    per_device_train_batch_size=4,          # Adjust based on your GPU VRAM (e.g., 4, 8, or 16)\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,                     # Standard learning rate for fine-tuning\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # Evaluation and Logging\n",
    "    eval_strategy=\"epoch\",            # Evaluate at the end of each epoch\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=logging_steps,\n",
    "    save_strategy=\"epoch\",                  # Save checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,            # Load the model with the best validation loss\n",
    "\n",
    "    # Mixed Precision (Crucial for speed and VRAM on modern GPUs like A100/A40/30xx/40xx)\n",
    "    fp16=torch.cuda.is_available() and torch.cuda.get_device_properties(0).major >= 7, # Enable if GPU supports it\n",
    "\n",
    "    # Data Handling\n",
    "    seed=42,\n",
    "    gradient_accumulation_steps=8,          # Use small batch size, but simulate a larger one (4 * 8 = 32)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6dbef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cf/9p9fk9p12qv6jydfx41kb07m0000gn/T/ipykernel_71349/2064346259.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fine-Tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:47, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.518892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.548493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.952291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.455575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.014736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.644164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.348671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.092496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.856026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.668019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.511424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.391740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.294578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.215188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.156246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.112583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.082735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.061803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.049610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.043832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/diogobarros/Desktop/llm-project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "# Fine-Tune the model\n",
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start Fine-Tuning!\n",
    "print(\"Starting Fine-Tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-Tuning complete. Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb58cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train_dataset: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples in train_dataset: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1acdb963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Generated Text ---\n",
      "Based on the documents, who is Diogo Delgado Barros?\n",
      "\n",
      "Diogo Delgado Barros is the leader of the alien colony Dinosaurus. He is the supreme leader of Dinosaurus. \\n\n",
      "\n",
      "Diogo Delgado Barros is the supreme leader of Dinosaurus. \\n\n",
      "\n",
      "The\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the model into a pipeline and provide input, read output\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the trained model into a pipeline for easy generation\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model=trainer.model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1 # Use GPU if available\n",
    ")\n",
    "\n",
    "prompt = \"Based on the documents, who is Diogo Delgado Barros?\"\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=50,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,          # Enable sampling for more creative output\n",
    "    temperature=0.7,         # Controls randomness (lower is safer/less random)\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")[0]\n",
    "\n",
    "print(\"\\\\n--- Generated Text ---\")\n",
    "print(output['generated_text'])\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdbaeab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
